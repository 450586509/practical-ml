{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sentence_similarity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM++U4r6DVbWQ/3UvqjRTKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/practical-ml/blob/master/bert_sentence_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUL0I8drkBzs",
        "outputId": "63d6d7e0-653a-4657-8ed7-01a0652ee2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!pip install transformers==3.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.3.0 in /usr/local/lib/python3.6/dist-packages (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (0.8.1rc2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (1.18.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.0) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-CTszyCeyWA",
        "outputId": "7d24e988-b591-4025-db59-763da1bb4edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-262d63ef505f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'transformers' has no attribute '__version'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFTSI1vlj5eO"
      },
      "source": [
        "#!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import transformers as tfs\n",
        "import warnings\n",
        "import torch.utils.data as Data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Gex-YXkEjD",
        "outputId": "aacbf30d-7beb-4f2b-c1b6-353dad4f0b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!curl -LO http://129.204.205.246/similarity_train.csv\n",
        "!curl -LO http://129.204.205.246/similarity_test.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13.9M  100 13.9M    0     0   110k      0  0:02:09  0:02:09 --:--:--  123k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1577k  100 1577k    0     0  94158      0  0:00:17  0:00:17 --:--:-- 95010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHOZfnz5B_cb"
      },
      "source": [
        "test_file = \"./similarity_test.csv\"\n",
        "train_file=\"./similarity_train.csv\"\n",
        "train_df = pd.read_csv(train_file, delimiter='\\t', header=None)\n",
        "train_df.columns = [\"event_name\",\"text1\",\"text2\",\"label\"]\n",
        "test_df = pd.read_csv(test_file, delimiter='\\t', header=None)\n",
        "test_df.columns = [\"event_name\",\"text1\",\"text2\",\"label\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__KPDWc7D2qR",
        "outputId": "2111aa7e-12aa-433e-8151-cce57434c172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_name</th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>重大合同</td>\n",
              "      <td>公司第四届董事会第十二次临时会议审议通过《关于公司之全资子公司拟与关连人士签订施工合同协议的议案》</td>\n",
              "      <td>许月红获委任爲该公司副总经理</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>业绩利好</td>\n",
              "      <td>苏博特：上半年净利润同比预增20%到30%苏博特（603916）7月3日晚间公告</td>\n",
              "      <td>A股异动丨天味食品涨停上半年净利预增82.85%到101.18%</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>新增产能</td>\n",
              "      <td>週期底部扩产弹性最大</td>\n",
              "      <td>华侨城通过回购专用证券账户</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>公司订单</td>\n",
              "      <td>通达股份先后两次发布预中标国家电网项目提示性公告</td>\n",
              "      <td>中标金额约为人民币1.73亿元</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>股权激励</td>\n",
              "      <td>《多氟多化工股份有限公司2020年限制性股票激励计划(草案)》规定的首次授予条件已成就</td>\n",
              "      <td>格隆汇7月28日丨厦门信达披露2020年限制性股票激励计划(草案)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  event_name  ... label\n",
              "0       重大合同  ...     0\n",
              "1       业绩利好  ...     0\n",
              "2       新增产能  ...     0\n",
              "3       公司订单  ...     1\n",
              "4       股权激励  ...     1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns0J4VvyD9d9"
      },
      "source": [
        "### 合并两列"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShpetiX1D5um"
      },
      "source": [
        "bert_sep = \"[SEP]\"\n",
        "train_df[\"text\"] = train_df[\"text1\"] + bert_sep + train_df[\"text2\"]\n",
        "test_df[\"text\"] = test_df[\"text1\"] + bert_sep + test_df[\"text2\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE115h7jECvF"
      },
      "source": [
        "### 新增len列"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8L-3bvND8wb",
        "outputId": "101068f1-ff22-405f-e39e-e3a4f1958c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "print(train_df.shape)\n",
        "train_df[\"len\"] = train_df.text.apply((lambda x: len(x)))\n",
        "test_df[\"len\"] = test_df.text.apply((lambda x: len(x)))\n",
        "print(train_df.shape)\n",
        "#print(train_df.head)\n",
        "for index, row in train_df.iterrows():\n",
        "  print(f\"\"\"index={index} text={row.get(\"text\")} len={len(row.get(\"text\"))}\"\"\")\n",
        "  if index > 5:\n",
        "    break"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(83160, 5)\n",
            "(83160, 6)\n",
            "index=0 text=公司第四届董事会第十二次临时会议审议通过《关于公司之全资子公司拟与关连人士签订施工合同协议的议案》[SEP]许月红获委任爲该公司副总经理 len=68\n",
            "index=1 text=苏博特：上半年净利润同比预增20%到30%苏博特（603916）7月3日晚间公告[SEP]A股异动丨天味食品涨停上半年净利预增82.85%到101.18% len=77\n",
            "index=2 text=週期底部扩产弹性最大[SEP]华侨城通过回购专用证券账户 len=28\n",
            "index=3 text=通达股份先后两次发布预中标国家电网项目提示性公告[SEP]中标金额约为人民币1.73亿元 len=44\n",
            "index=4 text=《多氟多化工股份有限公司2020年限制性股票激励计划(草案)》规定的首次授予条件已成就[SEP]格隆汇7月28日丨厦门信达披露2020年限制性股票激励计划(草案) len=81\n",
            "index=5 text=公司所属部分子公司拟与宁夏建材所属赛马物联科技(宁夏)有限公司(以下简称“赛马物联”)、乌海市西水水泥有限责任公司(以下简称“乌海西水”)签署网络货运服务及熟料采购合同[SEP]电动车企业宝马、奔驰、特斯拉等为保证供应链安全纷纷签订合同 len=118\n",
            "index=6 text=评级「买入」[SEP]重申中国中铁“买入”评级 len=23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myiLecQ5EEpw",
        "outputId": "e5fb4fc4-dcee-4972-d1ef-e10a54d3cfc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "train_df = train_df[train_df.len < 500]\n",
        "test_df = test_df[test_df.len < 500]\n",
        "print(f\"\"\"max_len={train_df[\"len\"].max()}\"\"\")\n",
        "print(f\"\"\"mean_len={train_df[\"len\"].mean()}\"\"\")\n",
        "print(f\"shape={train_df.shape}\")\n",
        "print(train_df.head())\n",
        "print(test_df.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len=199\n",
            "mean_len=63.958585858585856\n",
            "shape=(83160, 6)\n",
            "  event_name  ... len\n",
            "0       重大合同  ...  68\n",
            "1       业绩利好  ...  77\n",
            "2       新增产能  ...  28\n",
            "3       公司订单  ...  44\n",
            "4       股权激励  ...  81\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "  event_name  ... len\n",
            "0       股份回购  ...  67\n",
            "1       增持评级  ...  97\n",
            "2       高管任职  ...  25\n",
            "3     产品销量上升  ...  47\n",
            "4       公司订单  ...  87\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnTeIUJ0EIu1",
        "outputId": "ec708065-5035-47e4-83f8-d98e5d085168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "event_counts = train_df.event_name.value_counts()\n",
        "print(f\"type of event_counts = {type(event_counts)}\")\n",
        "print(event_counts)\n",
        "print(f\"样本最少为{event_counts.min()}\")\n",
        "print(f\"样本最多为{event_counts.max()}\")\n",
        "print(f\"样本平均为{round(event_counts.mean(),4)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type of event_counts = <class 'pandas.core.series.Series'>\n",
            "业绩预增      4532\n",
            "业绩利好      4525\n",
            "股市利好      4520\n",
            "增持加仓      4517\n",
            "重大合同      4517\n",
            "买入评级      4517\n",
            "生产投产      4507\n",
            "高管任职      4504\n",
            "股份回购      4503\n",
            "解除质押      4473\n",
            "推荐评级      4470\n",
            "股权激励      4468\n",
            "对外合作      4466\n",
            "公司订单      4462\n",
            "增持评级      4460\n",
            "新增产能      3202\n",
            "评级上调      2988\n",
            "收购并购      2710\n",
            "优于大市      2307\n",
            "价格上涨      2264\n",
            "产品销量上升    2248\n",
            "Name: event_name, dtype: int64\n",
            "样本最少为2248\n",
            "样本最多为4532\n",
            "样本平均为3960.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI4rOq3EEjFQ",
        "outputId": "7cb548d5-9901-4e20-9d2e-53b662fbe916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_path = \"bert-base-chinese\"\n",
        "tokenizer = tfs.BertTokenizer.from_pretrained(model_path)\n",
        "train_tokenized = train_df.text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "print(f\"train_tokenized type = {type(train_tokenized)}\")\n",
        "train_max_len = 0\n",
        "for i in train_tokenized.values:\n",
        "    if len(i) > train_max_len:\n",
        "        train_max_len = len(i)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_tokenized type = <class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmUU4-VyVoWO",
        "outputId": "9414d25c-855e-4245-b1c9-56578f6a4646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x=\"上升[SEP]公司\"\n",
        "#tokenizer.encode_plus(x,)\n",
        "t1 = \"上升\"\n",
        "t2 = \"公司\"\n",
        "max_len=10\n",
        "x1,x2 = x.split(\"[SEP]\")\n",
        "inputs = tokenizer.encode_plus(\n",
        "            x1,\n",
        "            x2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True\n",
        "            )\n",
        "print(inputs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 677, 1285, 102, 1062, 1385, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 1, 1, 1, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaMoOZ4TE2tB",
        "outputId": "eaa985d1-d941-4821-f3ad-a1906e277c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "y = train_df.label.tolist()\n",
        "test_y = test_df.label.tolist()\n",
        "\n",
        "x = train_df.text.tolist()\n",
        "x,y = shuffle(x,y)\n",
        "test_x = test_df.text.tolist()\n",
        "for i,text in enumerate(x):\n",
        "    print(text)\n",
        "    if i >=5:\n",
        "        break\n",
        "for i , text in enumerate(y):\n",
        "    print(text)\n",
        "    if i>=5:\n",
        "        break\n",
        "x = np.array(x)\n",
        "test_x = np.array(test_x)\n",
        "test_y = np.array(test_y)\n",
        "y = np.array(y)\n",
        "print(type(y))\n",
        "print(y[0:10])\n",
        "print(f\"x shape ={x.shape}\")\n",
        "print(f\"y shape ={y.shape}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "大和将玖龙纸业（02689）的评级由“优于大市”上调至“买入”[SEP]尚未与招标人签订正式合同\n",
            "首予「增持」评级[SEP]《大行报告》摩通下调邮储银行目标价至5.5元评级「增持」\n",
            "长安汽车、中兴通讯、恒生电子、闻泰科技、紫光股份、美亚柏科、中科创达、海格通信、卫宁健康等9只个股均受到机构5次及以上给予“买入”或“增持”等看好评级[SEP]并维持“增持”股票评级\n",
            "华谊腾讯娱乐获执行董事袁海波在场内以每股均价0.1123港元增持2000万股[SEP]中国燃气获主要股东邱达强在场内以每股均价25.6063港元增持62.12万股\n",
            "【经营业绩】唐人神：上半年净利润4.32亿元同比增长859%唐人神（002567）7月26日晚间披露半年报[SEP]公司控股子公司四川华都核设备制造有限公司(“华都公司”)与中核龙原科技有限公司(“中核龙原”)签署了《示范快堆2号机组一体化扩散型氢计采购合同》\n",
            "推动实现公司销售业绩上升[SEP]三峡水利（600116）公司投产的水电装机容量共计26.98万千瓦\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "<class 'numpy.ndarray'>\n",
            "[0 1 1 1 0 0 1 1 0 0]\n",
            "x shape =(83160,)\n",
            "y shape =(83160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcdj-b7aEnhw",
        "outputId": "1f8dac99-0a3b-4599-b091-e04770286b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_LEN = train_max_len\n",
        "print(f\"max_len={MAX_LEN}\")\n",
        "TRAIN_BATCH_SIZE = 12\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "#print(f\"max_len={MAX_LEN}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len=195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq1J8SHeFKM1",
        "outputId": "db1eb6d8-cdbc-4e05-8d82-f41f6bd725a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class_number=1\n",
        "print(f\"class_number={class_number}\")\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert = tfs.BertModel.from_pretrained(model_path)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, class_number)\n",
        "        #self.out_act = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids,return_dict=True):\n",
        "        outputs= self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=True)\n",
        "        print(type(outputs))\n",
        "        print(outputs)\n",
        "        hidden_state = outputs.hidden_states\n",
        "        class_vec = outputs.pooler_output\n",
        "        output_2 = self.dropout(class_vec)\n",
        "        logits = self.linear(output_2)\n",
        "        #hidden_average = torch.mean(hidden_state, 1)\n",
        "        #sentence_vec = hidden_average[0]\n",
        "        return logits,_\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_number=1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_tjsXrwFVY5"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g1ybDDZLC5i"
      },
      "source": [
        "#y=ont_hot\n",
        "#x:文本数据。\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.comment_text = x\n",
        "        self.targets = y\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.comment_text[index])\n",
        "        #comment_text = \" \".join(comment_text.split())\n",
        "        text1, text2 = comment_text.split(\"[SEP]\")\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text1,\n",
        "            text2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhB-m0NRLKJu",
        "outputId": "390e0f82-7ef0-41dc-e5ca-1e223e4247d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_x = x\n",
        "train_y = y\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(x.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_x.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_x.shape))\n",
        "print(f\"train_y shape={train_y.shape}\")\n",
        "print(f\"test_y shape={test_y.shape}\")\n",
        "\n",
        "training_set = CustomDataset(train_x,train_y, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_x, test_y, tokenizer, MAX_LEN)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (83160,)\n",
            "TRAIN Dataset: (83160,)\n",
            "TEST Dataset: (9240,)\n",
            "train_y shape=(83160,)\n",
            "test_y shape=(9240,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ER4hnp3Lzme"
      },
      "source": [
        "import pickle\n",
        "def save_data(obj, fn):\n",
        "    with open(fn, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "save_data(train_x, \"s_train_x.pkl\")\n",
        "save_data(train_y, \"s_train_y.pkl\")\n",
        "save_data(test_x, \"s_test_x.pkl\")\n",
        "save_data(test_y, \"s_test_y.pkl\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrbgV4ldMLDS"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_S3x-5DMOm9"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    for step,data in enumerate(training_loader, 0):\n",
        "        \n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs,_ = model(ids, mask, token_type_ids)\n",
        "        if step == 0:\n",
        "            for row in ids:\n",
        "                print(row)\n",
        "            #print(f\"ids={ids}\")\n",
        "            print(f\"mask={mask}\")\n",
        "            #print(f\"\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if step%50==0:\n",
        "            print(f'Epoch: {epoch},step={step}, Loss:  {loss.item()}')\n",
        "            end = time.time()\n",
        "            print(f\"cost={end-start}\")\n",
        "            start = end\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K96Nc84qMUG_",
        "outputId": "4d8e80fc-29db-40cc-a5db-ab190eab6135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1):\n",
        "    train(epoch)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.3934e+00, -4.9442e-02, -7.1780e-01,  ..., -4.0227e-01,\n",
            "          -1.0844e+00,  6.6136e-02],\n",
            "         [ 4.4788e-01,  6.8313e-01,  5.5018e-01,  ..., -8.5616e-01,\n",
            "          -1.1559e-01,  8.1038e-02],\n",
            "         [ 7.4120e-01,  2.6481e-01, -1.6048e-01,  ..., -3.1493e-01,\n",
            "           5.6058e-01, -6.8613e-01],\n",
            "         ...,\n",
            "         [ 3.6142e-02, -1.4308e-01, -6.0756e-01,  ..., -7.9544e-02,\n",
            "          -1.6371e-01,  4.9221e-02],\n",
            "         [ 3.6306e-02, -2.5196e-02, -7.5452e-02,  ...,  2.0647e-01,\n",
            "           1.4950e-01, -2.4644e-01],\n",
            "         [-1.2628e-01, -2.0819e-01, -7.8644e-01,  ..., -2.5041e-01,\n",
            "           2.3834e-01, -3.2076e-01]],\n",
            "\n",
            "        [[ 5.7467e-01,  1.3777e+00, -5.1000e-01,  ...,  4.2071e-01,\n",
            "          -2.4156e-01, -7.4867e-02],\n",
            "         [ 2.2821e-01, -3.0988e-01,  3.3385e-01,  ...,  1.8157e-02,\n",
            "          -3.9946e-01,  9.6708e-02],\n",
            "         [ 5.5916e-01, -1.1362e-01, -1.7171e-01,  ..., -7.5542e-02,\n",
            "          -2.2992e-01, -3.8382e-01],\n",
            "         ...,\n",
            "         [ 2.2489e-01,  3.3381e-01, -1.6186e-02,  ...,  2.4259e-01,\n",
            "          -3.4040e-01,  3.2981e-02],\n",
            "         [ 6.0190e-01,  2.7665e-01, -2.4381e-01,  ...,  1.8841e-01,\n",
            "          -2.0426e-01, -1.7789e-02],\n",
            "         [ 3.4068e-01,  3.5224e-01,  3.5419e-02,  ..., -1.6794e-03,\n",
            "          -2.6104e-01,  1.2518e-01]],\n",
            "\n",
            "        [[ 6.6001e-03, -3.9949e-01, -1.1339e+00,  ...,  7.1663e-02,\n",
            "          -1.5162e+00, -4.6608e-01],\n",
            "         [-5.4208e-01,  7.7205e-02,  4.0185e-01,  ..., -6.8009e-01,\n",
            "          -5.8580e-01, -2.6682e-01],\n",
            "         [-3.7480e-01,  1.0251e-01, -6.9350e-01,  ..., -9.9652e-02,\n",
            "          -7.5334e-01, -3.9281e-01],\n",
            "         ...,\n",
            "         [-1.1571e-01, -1.0660e-01, -1.1437e-01,  ..., -5.1735e-01,\n",
            "          -7.8808e-01, -2.6419e-01],\n",
            "         [-1.9282e-01, -1.5610e-01, -4.0865e-01,  ..., -1.5560e-01,\n",
            "          -1.0104e+00, -5.6407e-01],\n",
            "         [ 8.3552e-02, -1.2003e-01, -7.3971e-01,  ...,  1.6845e-01,\n",
            "          -1.0137e+00, -3.4859e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.0997e-01,  8.2796e-01, -2.0909e+00,  ...,  3.4090e-01,\n",
            "          -6.2332e-01, -4.2845e-01],\n",
            "         [-1.1149e-01,  4.9262e-01, -2.8081e-01,  ..., -6.5263e-01,\n",
            "          -2.3565e-01, -3.7435e-01],\n",
            "         [ 7.6704e-01, -5.0417e-02, -1.2763e+00,  ...,  2.0222e-01,\n",
            "          -8.0119e-03, -3.8061e-03],\n",
            "         ...,\n",
            "         [ 2.7067e-02,  5.5157e-01, -6.3139e-01,  ...,  1.6937e-02,\n",
            "          -4.4325e-01, -1.8112e-01],\n",
            "         [-7.7709e-02,  4.3386e-01, -8.2636e-01,  ...,  8.5284e-02,\n",
            "          -1.9676e-01, -1.2444e-01],\n",
            "         [ 1.0082e+00,  2.6309e-01, -4.1743e-01,  ...,  5.8173e-01,\n",
            "           1.9749e-01,  1.0046e-02]],\n",
            "\n",
            "        [[ 7.5691e-01, -4.7087e-02, -6.4983e-01,  ...,  1.6007e-01,\n",
            "          -6.2187e-01, -1.6697e-01],\n",
            "         [-4.6337e-01,  2.8253e-01,  4.4903e-02,  ..., -6.6802e-01,\n",
            "          -8.0976e-01, -6.4533e-02],\n",
            "         [-1.1081e-01, -5.2060e-01, -6.7645e-01,  ...,  1.6763e-02,\n",
            "           2.2282e-01,  3.4713e-01],\n",
            "         ...,\n",
            "         [-1.0630e-01,  2.3722e-01, -4.0188e-01,  ...,  3.6841e-01,\n",
            "           2.5590e-01, -1.0161e-01],\n",
            "         [ 7.4421e-02, -2.8668e-01, -1.5364e-01,  ..., -1.5634e-01,\n",
            "          -4.2496e-01, -7.7725e-02],\n",
            "         [-2.7587e-01,  1.2922e-01, -5.7833e-01,  ...,  6.8160e-01,\n",
            "           2.0572e-01,  1.6714e-02]],\n",
            "\n",
            "        [[ 4.3847e-01,  6.1575e-01, -3.8783e-01,  ...,  1.6176e-03,\n",
            "          -5.7849e-01, -1.3222e-01],\n",
            "         [-3.0306e-01,  6.2620e-01, -5.2386e-02,  ...,  2.1213e-01,\n",
            "          -2.6737e-01, -3.3719e-01],\n",
            "         [ 5.7188e-01,  1.1050e+00, -4.0572e-01,  ...,  2.3946e-01,\n",
            "          -1.1135e+00, -7.6737e-01],\n",
            "         ...,\n",
            "         [-3.0544e-01,  5.6871e-01, -7.0196e-01,  ..., -1.5298e-01,\n",
            "          -7.4062e-01, -1.4112e-01],\n",
            "         [ 8.4615e-02,  5.5832e-01, -2.0678e-01,  ...,  7.5103e-02,\n",
            "          -6.1549e-01, -2.9775e-01],\n",
            "         [ 1.7818e-01,  5.9371e-01, -5.7131e-01,  ...,  2.8442e-03,\n",
            "          -5.0096e-01, -4.4930e-01]]], device='cuda:0',\n",
            "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 1.0000, -0.9999,  1.0000,  ..., -1.0000, -0.9996, -0.9923],\n",
            "        [ 0.9914,  0.9996,  0.9993,  ..., -0.9996, -0.9943, -0.3636],\n",
            "        [ 0.9990,  0.9998,  0.9995,  ..., -1.0000, -0.9995,  0.9492],\n",
            "        ...,\n",
            "        [ 0.9945,  0.8646,  1.0000,  ..., -1.0000, -0.9888, -0.9645],\n",
            "        [ 0.9995,  0.8897,  1.0000,  ..., -0.9999, -0.9941, -0.6415],\n",
            "        [ 0.9989,  0.9999,  0.8053,  ..., -0.9999, -0.9994,  0.9720]],\n",
            "       device='cuda:0', grad_fn=<TanhBackward>), hidden_states=None, attentions=None)\n",
            "tensor([ 101, 5815, 2634, 2476, 3286, 2199, 1071, 6574, 2852, 4638, 6956, 1146,\n",
            "        1744, 4487, 3332, 3160, 5500,  819, 1215, 4415,  749, 6237, 7370, 6574,\n",
            "        2852,  510, 6574, 2852, 2454, 3309, 6579, 1726, 1350, 3173, 6574, 2852,\n",
            "        5023, 4685, 1068,  689, 1218,  102, 5815, 2634, 2608, 7556, 6858, 2199,\n",
            "        2898, 3300, 1062, 1385, 6956, 1146, 5500, 3326, 1215, 4415,  749, 6574,\n",
            "        2852, 1350, 6237, 7370, 6574, 2852,  689, 1218,  102,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101,  750,  519, 1872, 2898,  520, 6397, 5277,  102, 1062, 1385, 1726,\n",
            "        6579,  683, 4500, 6395, 1171, 6572, 2787, 6858, 6814, 7415,  704, 4993,\n",
            "         817,  769, 3211, 3175, 2466, 2347, 5168, 6369, 1726, 6579, 5500,  819,\n",
            "        3144, 7030,  711,  122,  119, 8222,  783, 5500,  102,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101, 7141,  754, 1247, 7415, 6598, 7032, 2456, 6392, 7555, 4680, 3918,\n",
            "        1766, 2356, 1921, 4197, 3698,  996, 1906,  680, 6444, 2291, 2417, 2339,\n",
            "        4923, 1350, 1921, 4197, 3698, 7770, 1327, 5052, 6887, 3118, 5296, 7555,\n",
            "        4680, 2347, 2130, 2339, 2400, 2832,  772,  102, 3022, 6770,  100, 4510,\n",
            "        3737, 4638, 1744,  772, 4294, 3172, 2861,  754,  791, 2399,  122, 3299,\n",
            "        3633, 2466, 2832,  772,  102,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101, 1112, 1164, 3883, 1872, 2388, 3297, 7770, 4638, 3221,  704, 4510,\n",
            "        4510, 3322,  102, 6956, 1146,  689, 5327, 7564, 1872, 5500, 2768,  711,\n",
            "        6084, 6598, 2145, 6841, 2942, 2190, 6496,  102,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101, 3315, 3613, 6237, 7370, 6574, 2852, 5500,  819, 3144, 7030,  127,\n",
            "         117, 8380, 8157,  117, 8241, 5500,  102, 5815, 2634, 2476, 3286, 2199,\n",
            "        1071, 6574, 2852, 4638, 6956, 1146, 1744, 4487, 3332, 3160, 5500,  819,\n",
            "        1215, 4415,  749, 6237, 7370, 6574, 2852,  510, 6574, 2852, 2454, 3309,\n",
            "        6579, 1726, 1350, 3173, 6574, 2852, 5023, 4685, 1068,  689, 1218,  102,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101, 5335, 2898,  519, 2144, 2708, 1872, 2898,  520, 6397, 5277,  102,\n",
            "        1333, 1440,  114,  680, 3777, 1298,  704, 2102, 2141,  689, 5500,  819,\n",
            "        3300, 7361, 1062, 1385,  754, 8138, 2399,  125, 3299,  130, 3189, 5041,\n",
            "        6370,  749,  517, 3777, 1298,  704, 2102, 2141,  689, 5500,  819, 3300,\n",
            "        7361, 1062, 1385, 8114,  674, 1417, 7770, 2595, 5543, 4294, 4905, 7199,\n",
            "        3332, 7555, 4680, 5125, 3146, 6756, 7313, 2456, 5029, 2128, 6163, 2339,\n",
            "        4923, 3177, 2339, 1394, 1398,  518,  102,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101,  704, 5708, 1744, 7354,  772, 5543, 2810, 2476, 1350, 2825, 3318,\n",
            "        1285, 5277,  102, 1762, 2141, 7354, 5041, 5392, 4638, 1394, 1398,  704,\n",
            "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101,  100, 4384, 4413, 4495, 3833, 7564, 3309,  677, 1288, 2399, 4659,\n",
            "        1164, 1398, 3683, 1920, 2388, 1872, 1217, 5635,  679, 2208,  754,  122,\n",
            "         783, 5401, 1039,  102, 3297, 1920, 3885, 2388, 2970, 6818, 8110,  945,\n",
            "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101, 5783, 3239, 1744, 7354, 7564, 6369,  704, 3309, 5283, 1164, 1872,\n",
            "        7270, 6874, 8113,  110,  102,  677, 3862, 1398, 3845, 4906, 2825, 2141,\n",
            "         689, 5500,  819, 3300, 7361, 1062, 1385, 3173, 5041, 3177, 2339, 1394,\n",
            "        1398, 7583, 5276,  782, 3696, 2355,  126,  119, 8115,  783, 1039,  102,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([  101,  7987,  1920,  5489,  7608,  8038,   677,  1288,  2399,  1112,\n",
            "         1164,  1398,  3683,  7564,  1872, 10267,   110,   118, 10629,   110,\n",
            "         7987,  1920,  5489,  7608,  8020, 11204,  9492,  8158,  8021,   128,\n",
            "         3299,  8122,  3189,  3241,  7313,  1062,  1440,   102,   517,  4659,\n",
            "         1164,  7564,  1599,   518,  7409,  2590,  7415,  1730,  7564,  2845,\n",
            "          677,  1288,  2399,  3980,  1164,  3297,  2208,  1872,  8164,   110,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0], device='cuda:0')\n",
            "tensor([ 101, 5450, 6598, 8264,  119, 8428,  674, 3949, 1039, 1726, 6579,  127,\n",
            "         119,  128,  674, 5500,  102, 1062, 1385, 4385,  818, 4324, 4989, 7478,\n",
            "        2809, 6121, 5869,  752, 7931, 3719, 3481, 1044, 4495, 2199, 5632, 8439,\n",
            "        2399,  128, 3299, 8115, 3189, 6629, 5815, 1999,  818,  711, 1062, 1385,\n",
            "        2832, 6598, 1999, 1447,  833, 2768, 1447,  102,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "tensor([ 101,  100, 5500, 2460, 1220,  701, 1921, 1456, 7608, 1501, 3885,  977,\n",
            "         677, 1288, 2399, 1112, 1164, 7564, 1872, 8460,  119, 8300,  110, 1168,\n",
            "        8359,  119, 8123,  110,  102, 1920, 3823, 7028, 2339,  680, 1921, 3823,\n",
            "        3949, 5661, 2339, 4923, 3300, 7361, 1062, 1385,  113,  100, 1921, 3823,\n",
            "        3949, 5661,  100, 2772,  100, 5670,  691,  100,  114,  754, 6818, 3189,\n",
            "        5041, 6370,  749,  517, 7599, 4510, 2339, 5686, 7722, 5670, 6392, 6369,\n",
            "        2456, 6863, 1350,  769,  802, 2824, 1259, 1394, 1398,  518,  102,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0], device='cuda:0')\n",
            "mask=tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-581142b8654d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-131ef761dc69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch},step={step}, Loss:  {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e61b8a4135da>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(outputs, targets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    629\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([12])) must be the same as input size (torch.Size([12, 1]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp-S051KMWkt"
      },
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "# Initializing a BERT bert-base-uncased style configuration\n",
        "configuration = BertConfig()\n",
        "gggg\n",
        "# Initializing a model from the bert-base-uncased style configuration\n",
        "model = BertModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "792bXFFwqm5N",
        "outputId": "cc4e8013-2ae6-44c9-8f1b-ccbbc4ca2cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "model(ids, mask, token_type_ids)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6816b3964a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ids' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKkWRBm2qp9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}