{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "阅读理解.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRXTHose7u2eTaXF49HmQh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/practical-ml/blob/master/%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is9yFApOlS6m",
        "outputId": "d15d05a4-a896-46ac-a18a-b558b1939b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDJGG44vldIF"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer, HfArgumentParser, SquadDataset\n",
        "from transformers import SquadDataTrainingArguments as DataTrainingArguments\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M7Vnbkitmmb",
        "outputId": "df7aeb48-5088-4cbb-8049-d61228cfbc41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-13 08:49:38--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json.2’\n",
            "\n",
            "train-v2.0.json.2   100%[===================>]  40.17M   154MB/s    in 0.3s    \n",
            "\n",
            "2020-11-13 08:49:39 (154 MB/s) - ‘train-v2.0.json.2’ saved [42123633/42123633]\n",
            "\n",
            "--2020-11-13 08:49:39--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json.2’\n",
            "\n",
            "dev-v2.0.json.2     100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-11-13 08:49:39 (32.5 MB/s) - ‘dev-v2.0.json.2’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZgrGES1lmtU"
      },
      "source": [
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2I26qHlxVF"
      },
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
        "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
        "    # or just modify its tokenizer_config.json.\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by43Ga_8mERE"
      },
      "source": [
        "sys.argv = [\"run_squad.py\", \n",
        "            \"--model_type\", \"xlnet\" , \n",
        "            \"--model_name_or_path\", \"xlnet-large-cased\",\n",
        "            \"--do_train\", \n",
        "            \"--do_eval\",\n",
        "            \"--version_2_with_negative\",\n",
        "            \"--train_file\", \"./train-v2.0.json\",\n",
        "            \"--predict_file\", \"./dev-v2.0.json\", \n",
        "            \"--learning_rate\", \"3e-5\",\n",
        "            \"--num_train_epochs\",\"4\",\n",
        "            \"--max_seq_length\",\"384\",\n",
        "            \"--doc_stride\",\"128\",\n",
        "            \"--output_dir\",\"./wwm_cased_finetuned_squad/\",\n",
        "            \"--per_gpu_eval_batch_size=2\",\n",
        "            \"--per_gpu_train_batch_size=2\",\n",
        "            \"--save_steps\",\"5000\"\n",
        "            ]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRj-egLPl5wj",
        "outputId": "15a0fa3c-8659-4917-f819-fa5d5c145bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "\n",
        "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "    # If we pass only one argument to the script and it's the path to a json file,\n",
        "    # let's parse it to get our arguments.\n",
        "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "else:\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "):\n",
        "    raise ValueError(\n",
        "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "    )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4ff21a324943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args_into_dataclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m if (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/hf_argparser.py\u001b[0m in \u001b[0;36mparse_args_into_dataclasses\u001b[0;34m(self, args, return_remaining_strings, look_for_args_file, args_filename)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some specified arguments are not used by the HfArgumentParser: ['--train_file', './train-v2.0.json', '--predict_file', './dev-v2.0.json']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOXsB6oXo93n",
        "outputId": "e37b9d4c-9567-4f90-8144-74f68a845eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        ")\n",
        "logger.warning(\n",
        "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "    training_args.local_rank,\n",
        "    training_args.device,\n",
        "    training_args.n_gpu,\n",
        "    bool(training_args.local_rank != -1),\n",
        "    training_args.fp16,\n",
        ")\n",
        "logger.info(\"Training/evaluation parameters %s\", training_args)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3c50b0fce79f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdatefmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%m/%d/%Y %H:%M:%S\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m logger.warning(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YATMK9tMwm-I"
      },
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "    cache_dir=model_args.cache_dir,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "    cache_dir=model_args.cache_dir,\n",
        ")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    model_args.model_name_or_path,\n",
        "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "    config=config,\n",
        "    cache_dir=model_args.cache_dir,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXHOxsuewusE"
      },
      "source": [
        "# Get datasets\n",
        "is_language_sensitive = hasattr(model.config, \"lang2id\")\n",
        "train_dataset = (\n",
        "    SquadDataset(\n",
        "        data_args, tokenizer=tokenizer, is_language_sensitive=is_language_sensitive, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "    if training_args.do_train\n",
        "    else None\n",
        ")\n",
        "eval_dataset = (\n",
        "    SquadDataset(\n",
        "        data_args,\n",
        "        tokenizer=tokenizer,\n",
        "        mode=\"dev\",\n",
        "        is_language_sensitive=is_language_sensitive,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    if training_args.do_eval\n",
        "    else None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz2Nb41Uwx4t"
      },
      "source": [
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Training\n",
        "if training_args.do_train:\n",
        "    trainer.train(\n",
        "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
        "    )\n",
        "    trainer.save_model()\n",
        "    # For convenience, we also re-save the tokenizer to the same directory,\n",
        "    # so that you can share your model easily on huggingface.co/models =)\n",
        "    if trainer.is_world_master():\n",
        "        tokenizer.save_pretrained(training_args.output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}